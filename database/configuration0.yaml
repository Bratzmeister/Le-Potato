GENERAL:
  # General settings for the web server
  host: 
  port: 1234
  debug: False
  backend: True
  backend_verbose: False # Display more details about the backend

  # General settings for the model inference | Required either self host or api host (tested with Openai compatible endpoints and Groq)
  # Check some content about the samplers and parameters if you need more information

  hosting_method: openai # ( openai compatible endpoints only); (you can use 'custom' for a custom implementation of the inference method following the OpenAIInference class...)
  online_model_name: "llama3-70b-8192" # openai model here
  online_model_base_url: "https://api.groq.com/openai/v1" # openai api base url
  online_model_api_key: "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" # api key of your api

  rerank_top_n: 3
  similarity_top_k: 6
  rag_chunk_size: 500
  rag_chunk_overlap: 50
  embedding_model_dimensions: 1024 # Dimensions of the embedding model
  embedding_model: "mixedbread-ai/mxbai-embed-large-v1" # Embedding model from huggingface
  reranker_model: "mixedbread-ai/mxbai-rerank-xsmall-v1" # Reranker model from huggingface

  summarize_web_content: False # Summarize the web content before sending to the model
  summarize_model: "pszemraj/long-t5-tglobal-base-16384-book-summary" # Summarizer model from huggingface

  display_chat_history: False # Display chat history on the terminal
  
  username: "Potato"
  model_username: "Julianne"

MODEL_INFERENCE_PARAMS:
  # Parameters for model inference
  model_system_prompt: "Below is a conversation between Julianne (you) which have the ability to access user files once user upload it and a curious user" # leave it empty for default
  model_default_top_p: 0.75
  model_default_temperature: 0.85

  model_default_max_tokens: 512
  model_default_stop_sequence: ["<s>", "</s>", "user:", "assistant"]

  model_default_frequency_penalty: 0
  model_default_presence_penalty: 0
